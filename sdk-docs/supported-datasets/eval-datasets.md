# Eval Datasets

Evaluation datasets are used to benchmark and assess the performance of AI coding agents on specific tasks. They provide a standardized set of problems to measure an agent's ability to solve bugs, implement features, or perform other software engineering tasks.

| Name | Type of Tasks | # Samples | # Repos | Language(s) | Availability |
| --- | --- | --- | --- | --- | --- |
| [GitBug-Java](https://github.com/gitbugactions/gitbug-java) | Bug-fixing | 199 | 55 | Java, Python | <span style="color:grey;">Planned</span> |
| [SWE-bench](https://github.com/SWE-bench/SWE-bench) | Bug-fixing, feature addition | 2,294 | 12 | Python | <span style="color:grey;">Planned</span> |
